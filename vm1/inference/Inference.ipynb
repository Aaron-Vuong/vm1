{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Enable saving to GDrive for long running jobs.\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "# https://stackoverflow.com/a/79469725\n",
        "# 1. Mount Google Drive for output storage\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UoJ0R5aaWAp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13VS8A83S8Hc"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages for inference\n",
        "!pip install  -U -q transformers==4.51.3 git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
        "# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1\n",
        "!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"avuong/vm1\"\n",
        "\n",
        "# Load Dataset\n",
        "dataset = load_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "rgaGQVWGT0aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/foreverbeliever/OmniMedVQA/resolve/main/OmniMedVQA.zip\n",
        "!unzip OmniMedVQA\n",
        "!rm OmniMedVQA.zip"
      ],
      "metadata": {
        "id": "YEEMFd5sT3-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "# import time\n",
        "\n",
        "\n",
        "# def clear_memory():\n",
        "#     # Delete variables if they exist in the current global scope\n",
        "#     if \"inputs\" in globals():\n",
        "#         del globals()[\"inputs\"]\n",
        "#     if \"model\" in globals():\n",
        "#         del globals()[\"model\"]\n",
        "#     if \"processor\" in globals():\n",
        "#         del globals()[\"processor\"]\n",
        "#     if \"trainer\" in globals():\n",
        "#         del globals()[\"trainer\"]\n",
        "#     if \"peft_model\" in globals():\n",
        "#         del globals()[\"peft_model\"]\n",
        "#     if \"bnb_config\" in globals():\n",
        "#         del globals()[\"bnb_config\"]\n",
        "#     time.sleep(2)\n",
        "\n",
        "#     # Garbage collection and clearing CUDA memory\n",
        "#     gc.collect()\n",
        "#     time.sleep(2)\n",
        "#     torch.cuda.empty_cache()\n",
        "#     torch.cuda.synchronize()\n",
        "#     time.sleep(2)\n",
        "#     gc.collect()\n",
        "#     time.sleep(2)\n",
        "\n",
        "#     print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "#     print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "# clear_memory()"
      ],
      "metadata": {
        "id": "F_EfJ2vouFvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, AutoTokenizer\n",
        "\n",
        "MODEL_PATH = \"avuong/vm1-sft\"\n",
        "BSZ = 1\n",
        "MAX_THINK_TOKENS = 2048\n",
        "OUTPUT_PATH = f\"output_inference_{MAX_THINK_TOKENS}.json\"\n",
        "input_json = \"test_split.json\"\n",
        "\n",
        "all_outputs = []\n",
        "\n",
        "def extract_option_answer(output_str):\n",
        "    if len(output_str) == 1:\n",
        "        return output_str\n",
        "    # Try to find the number within <answer> tags, if can not find, return None\n",
        "    answer_pattern = r'<answer>\\s*(\\w+)\\s*</answer>'\n",
        "    match = re.search(answer_pattern, output_str)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "def extract_think_text(output_str):\n",
        "    # Try to find the number within <think> tags, if can not find, return None\n",
        "    answer_pattern = r'(.*)\\s*</think>'\n",
        "    match = re.search(answer_pattern, output_str)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "def remove_special_tokens(output_str):\n",
        "    # Take the thinking process and remove the end-of-think token/answers.\n",
        "    cleaned_str = output_str.split(\"</think>\")[0]\n",
        "    return cleaned_str\n",
        "\n",
        "final_output = []\n",
        "correct_number = 0\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    #attn_implementation=\"flash_attention_2\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side=\"right\")\n",
        "processor.tokenizer.padding_side = \"right\"\n",
        "\n",
        "BASE_PATH=\"OmniMedVQA/\"\n",
        "NUM_TO_EVALUATE = 40\n",
        "MIN_TOKENS_PER_ITER = 0\n",
        "IGNORE_STR = \"Wait\"\n",
        "SPECIAL_TOKENS=r\"</think>|<answer>|</answer>\"\n",
        "MAX_NUM_IGNORES=0\n",
        "repick_questions = False\n",
        "QUESTION_TEMPLATE = \"{Question} Output the thinking process in <think> </think>, think for only {ThinkTokens} tokens, and output a single, final single-letter choice (A, B, C, D ...) in <answer> </answer> tags.\"\n",
        "\n",
        "messages = []\n",
        "\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "def generate_random_test_set(test_data):\n",
        "    import random\n",
        "    data = []\n",
        "    evaluate_count = 0\n",
        "    available_indices = set(list(range(0, len(test_data))))\n",
        "    chosen_elements = []\n",
        "    while evaluate_count < NUM_TO_EVALUATE:\n",
        "        chosen_index = random.choice(list(available_indices))\n",
        "        chosen_elements.append(chosen_index)\n",
        "        available_indices.remove(chosen_index)\n",
        "        evaluate_count += 1\n",
        "    print(chosen_elements)\n",
        "    return chosen_elements\n",
        "\n",
        "data = []\n",
        "\n",
        "for test_json in [input_json]:\n",
        "    t_data = []\n",
        "    with open(test_json, \"r\", encoding=\"utf-8\") as f:\n",
        "        test_data = json.load(f)\n",
        "\n",
        "        # Generate a random test set of questions.\n",
        "        if os.path.exists(\"test_set.json\"):\n",
        "            print(\"Loading a random test set.\")\n",
        "            with open(\"test_set.json\", \"r\") as f:\n",
        "                random_test_set = json.load(f)\n",
        "            if len(random_test_set) != NUM_TO_EVALUATE:\n",
        "                print(\"Evaluation number has changed, resampling indices to questions.\")\n",
        "                repick_questions = True\n",
        "        if not os.path.exists(\"test_set.json\") or repick_questions is True:\n",
        "            print(\"Generating a random test set.\")\n",
        "            random_test_set = generate_random_test_set(test_data)\n",
        "            with open(\"test_set.json\", \"w\") as f:\n",
        "                json.dump(random_test_set, f, indent=2)\n",
        "\n",
        "        # Augment the data with a new ID key so we can keep track.\n",
        "        for index in random_test_set:\n",
        "            req = test_data[index]\n",
        "            custom_id = f\"{os.path.basename(test_json)}_{index}\"\n",
        "            req['id'] = custom_id.replace(\" \", \"_\").replace(\".\", \"-\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "            t_data.append(req)\n",
        "    data.extend(t_data)\n",
        "\n",
        "for i in data:\n",
        "    message = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": f\"file://{BASE_PATH}{i['image']}\"\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": QUESTION_TEMPLATE.format(Question=i['problem'],\n",
        "                                                 ThinkTokens=MAX_THINK_TOKENS)\n",
        "            }\n",
        "        ]\n",
        "    }]\n",
        "    messages.append(message)\n",
        "\n",
        "for i in tqdm(range(0, len(messages))):\n",
        "    message = messages[i]\n",
        "    continue_generating = True\n",
        "    token_count = 0\n",
        "    max_new_tokens = MAX_THINK_TOKENS\n",
        "    while continue_generating:\n",
        "        # Prompt the model to provide a final answer.\n",
        "        if token_count >= MAX_THINK_TOKENS and message[-1][\"role\"] == \"assistant\":\n",
        "            message[-1][\"content\"][0][\"text\"] = remove_special_tokens(message[-1][\"content\"][0][\"text\"])\n",
        "            message[-1][\"content\"][0][\"text\"] += \"</think><answer>\"\n",
        "            print(\"Prompting for final answer!\")\n",
        "            message.append({\n",
        "               \"role\": \"user\",\n",
        "               \"content\": \"Do not reason further. Provide a final, single-letter (A, B, C, D ...) answer in <answer></answer> tags. FINAL ANSWER:\"\n",
        "            })\n",
        "            max_new_tokens = 25\n",
        "            continue_generating = False\n",
        "        # Append the Wait token if we haven't reached the thinking limit.\n",
        "        if token_count < MAX_THINK_TOKENS and message[-1][\"role\"] == \"assistant\":\n",
        "            message[-1][\"content\"][0][\"text\"] = remove_special_tokens(message[-1][\"content\"][0][\"text\"])\n",
        "            message[-1][\"content\"][0][\"text\"] += IGNORE_STR\n",
        "            max_new_tokens = MAX_THINK_TOKENS - token_count\n",
        "\n",
        "        # Preparation for inference\n",
        "        image_inputs, video_inputs = process_vision_info(message)\n",
        "        text = [processor.apply_chat_template(message, tokenize=False, add_generation_prompt=True)]\n",
        "        inputs = processor(\n",
        "            text=text,\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "\n",
        "        # Inference: Generation of the output\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            use_cache=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False)\n",
        "\n",
        "        generated_ids_trimmed = [\n",
        "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        batch_output_text = processor.batch_decode(\n",
        "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "        )\n",
        "        print(json.dumps(batch_output_text, indent=2))\n",
        "        # Get the token count of the output_text.\n",
        "        token_count += len(tokenizer.encode(batch_output_text[0], add_special_tokens=False))\n",
        "        print(f\"Tokens generated: {token_count}\")\n",
        "        if message[-1][\"role\"] != \"assistant\":\n",
        "            message.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [{\n",
        "                    \"type\": \"text\",\n",
        "                    # We only have one batch_output, so we can just index by that.\n",
        "                    \"text\": batch_output_text[0]\n",
        "                }]\n",
        "            })\n",
        "    all_outputs.extend(batch_output_text)\n",
        "    print(f\"Answer: {batch_output_text}\")\n",
        "    print(f\"Processed batch {i+1}/{len(messages)}\")\n",
        "\n",
        "for input_example, model_output in zip(data,all_outputs):\n",
        "    original_output = model_output\n",
        "    ground_truth = input_example['solution']\n",
        "    model_answer = extract_option_answer(original_output)\n",
        "\n",
        "    # Create a result dictionary for this example\n",
        "    result = {\n",
        "        'id': input_example['id'],\n",
        "        'question': input_example,\n",
        "        'ground_truth': ground_truth,\n",
        "        'model_output': original_output,\n",
        "        'extracted_answer': model_answer\n",
        "    }\n",
        "    final_output.append(result)\n",
        "\n",
        "    ground_truth_pattern = r'<answer>\\s*(\\w+)\\s*</answer>'\n",
        "    ground_truth_match = re.search(ground_truth_pattern, ground_truth)\n",
        "    ground_truth = ground_truth_match.group(1)\n",
        "\n",
        "    # Count correct answers\n",
        "    print(f\"model_answer: {model_answer}, ground_truth: {ground_truth}\")\n",
        "    if model_answer is not None and model_answer == ground_truth:\n",
        "        correct_number += 1\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = (correct_number / len(all_outputs)) * 100\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save results to a JSON file\n",
        "output_path = OUTPUT_PATH\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump({\n",
        "        'accuracy': accuracy,\n",
        "        'results': final_output\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {output_path}\")"
      ],
      "metadata": {
        "id": "lWR2DLOKTdyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 3. Get the filename\n",
        "filename = output_path\n",
        "\n",
        "# 4. Specify the destination path\n",
        "destination_path = os.path.join('/content/drive/My Drive/Colab_Output', filename)\n",
        "\n",
        "# 5. Move the uploaded file to the destination\n",
        "shutil.move(filename, destination_path)\n",
        "\n",
        "print(f'File \"{filename}\" uploaded to \"{destination_path}\" successfully.')"
      ],
      "metadata": {
        "id": "Oy5vC_fwWFg1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}