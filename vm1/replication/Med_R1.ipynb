{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "# https://stackoverflow.com/a/79469725\n",
        "# 1. Mount Google Drive for output storage\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S082bBQjY3Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U -q trl bitsandbytes peft hf_xet tensorboard\n",
        "!pip install  -U -q transformers==4.51.3 git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
        "# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1\n",
        "!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "wVLytyYYR3j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone just LFS pointers for Med-R1 models\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/yuxianglai117/Med-R1"
      ],
      "metadata": {
        "id": "cWhxw71yIM5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull only the Med-R1/VQA_CT model.\n",
        "# This should take about 6 minutes with the basic Colab runtime.\n",
        "!cd Med-R1/ && git lfs pull --include=\"VQA_CT\""
      ],
      "metadata": {
        "id": "5w77NsLhJS34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the OmniMedVQA dataset.\n",
        "!wget https://huggingface.co/datasets/foreverbeliever/OmniMedVQA/resolve/main/OmniMedVQA.zip\n",
        "!unzip OmniMedVQA\n",
        "!rm OmniMedVQA.zip"
      ],
      "metadata": {
        "id": "wAPg8VZfKQTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir gh\n",
        "!cd gh && git clone https://github.com/Yuxiang-Lai117/Med-R1.git"
      ],
      "metadata": {
        "id": "_-w-bflrMZNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3NFgNQCvcAa"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, AutoTokenizer\n",
        "\n",
        "MODEL_PATH = \"Med-R1/VQA_CT\"\n",
        "BSZ = 1\n",
        "OUTPUT_PATH = \"output.json\"\n",
        "modalities = glob.glob(\"Med-R1/Splits/modality/test/*.json\")\n",
        "question_types = glob.glob(\"Med-R1/Splits/question_type/test/*.json\")\n",
        "\n",
        "all_outputs = []\n",
        "\n",
        "def extract_option_answer(output_str):\n",
        "    # Try to find the number within <answer> tags, if can not find, return None\n",
        "    answer_pattern = r'<answer>\\s*(\\w+)\\s*</answer>'\n",
        "    match = re.search(answer_pattern, output_str)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "def extract_think_text(output_str):\n",
        "    # Try to find the number within <think> tags, if can not find, return None\n",
        "    answer_pattern = r'(.*)\\s*</think>'\n",
        "    match = re.search(answer_pattern, output_str)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "def remove_special_tokens(output_str, patterns):\n",
        "    return re.sub(patterns, '', output_str)\n",
        "\n",
        "final_output = []\n",
        "correct_number = 0\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    #attn_implementation=\"flash_attention_2\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side=\"left\")\n",
        "\n",
        "BASE_PATH=\"OmniMedVQA/\"\n",
        "MAX_THINK_TOKENS = 2048\n",
        "MIN_TOKENS_PER_ITER = 0\n",
        "IGNORE_STR = \"Wait\"\n",
        "SPECIAL_TOKENS=r\"<think>|</think>|<answer>|</answer>\"\n",
        "MAX_NUM_IGNORES=0\n",
        "\n",
        "QUESTION_TEMPLATE = \"{Question} First output the thinking process in <think> </think> and final choice (A, B, C, D ...) in <answer> </answer> tags.\"\n",
        "\n",
        "messages = []\n",
        "\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "data = []\n",
        "for test_json in [*modalities, *question_types]:\n",
        "    with open(test_json, \"r\", encoding=\"utf-8\") as f:\n",
        "        test_data = json.load(f)\n",
        "        # Augment the data with a new ID key so we can keep track.\n",
        "        for index, req in enumerate(test_data):\n",
        "            custom_id = f\"{os.path.basename(test_json)}_{index}\"\n",
        "            req['id'] = custom_id.replace(\" \", \"_\").replace(\".\", \"-\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "\n",
        "    data.extend(test_data)\n",
        "\n",
        "for i in data:\n",
        "    message = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": f\"file://{BASE_PATH}{i['image']}\"\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": QUESTION_TEMPLATE.format(Question=i['problem'])\n",
        "            }\n",
        "        ]\n",
        "    }]\n",
        "    messages.append(message)\n",
        "\n",
        "num_tokens = [0] * len(messages)\n",
        "\n",
        "for i in tqdm(range(0, len(messages), BSZ)):\n",
        "    continue_generating = True\n",
        "    num_ignores = 0\n",
        "    while continue_generating:\n",
        "        batch_messages = messages[i:i + BSZ]\n",
        "\n",
        "        # Preparation for inference\n",
        "        text = []\n",
        "        max_new_tokens = 256 #MIN_TOKENS_PER_ITER\n",
        "        for batch_idx, msg in enumerate(batch_messages):\n",
        "            if num_tokens[i+batch_idx] < MAX_THINK_TOKENS:\n",
        "                if msg[0].get(\"new_msg\"):\n",
        "                    print(f\"MSG new: {msg[0]['new_msg']}\")\n",
        "                    new_msg = \"<think>\" + msg[0][\"new_msg\"]\n",
        "                else:\n",
        "                    new_msg = \"\"\n",
        "                text.append(processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) + new_msg)\n",
        "                print(text)\n",
        "                if MAX_THINK_TOKENS - num_tokens[i+batch_idx] > max_new_tokens:\n",
        "                    max_new_tokens = max(MAX_THINK_TOKENS - num_tokens[i+batch_idx], MIN_TOKENS_PER_ITER)\n",
        "\n",
        "        print(f\"Max Tokens to generate: {max_new_tokens}\")\n",
        "        #text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]\n",
        "        image_inputs, video_inputs = process_vision_info(batch_messages)\n",
        "        inputs = processor(\n",
        "            text=text,\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "\n",
        "        # Inference: Generation of the output\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            use_cache=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False)\n",
        "\n",
        "        generated_ids_trimmed = [\n",
        "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        batch_output_text = processor.batch_decode(\n",
        "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "        )\n",
        "\n",
        "        # Apply the s1 methodology.\n",
        "        for batch_idx, output in enumerate(batch_output_text):\n",
        "            print(output)\n",
        "            model_think = extract_think_text(output)\n",
        "            # If the <think> section has not completed, we can just get the number of tokens of\n",
        "            # the whole output and keep going.\n",
        "            if not model_think:\n",
        "                print(\"Using output!\")\n",
        "                model_think = output\n",
        "\n",
        "            # Remove any <think>, </think>, <answer>, </answer>, ... tokens\n",
        "            model_think = remove_special_tokens(model_think, SPECIAL_TOKENS)\n",
        "            num_tokens[i + batch_idx] += len(tokenizer.encode(model_think, add_special_tokens=False))\n",
        "            print(f\"Tokens generated: {num_tokens[i+batch_idx]}\")\n",
        "\n",
        "        continue_generating = False\n",
        "        # Check if all prompts in the batch have reached the generation limit.\n",
        "        for num_token in num_tokens[i:i + BSZ]:\n",
        "            if num_token < MAX_THINK_TOKENS:\n",
        "                continue_generating = True\n",
        "\n",
        "        # If we set an upper-limit of ignores, stop multi-generation.\n",
        "        if num_ignores >= MAX_NUM_IGNORES:\n",
        "            continue_generating = False\n",
        "\n",
        "        for batch_idx, msg in enumerate(batch_messages[0]):\n",
        "            if num_tokens[i+batch_idx] < MAX_THINK_TOKENS:\n",
        "                # Append the new token to the end of the content.\n",
        "                print(f\"MSG: {msg}\")\n",
        "                if msg.get(\"new_msg\"):\n",
        "                    old_msg = msg[\"new_msg\"]\n",
        "                else:\n",
        "                    old_msg = \"\"\n",
        "\n",
        "                model_think = extract_think_text(batch_output_text[batch_idx])\n",
        "                # Don't keep going if there is no thinking section.\n",
        "                if not model_think:\n",
        "                    print(\"NO THINK!\")\n",
        "                    print(model_think)\n",
        "                    print(\"-----------\")\n",
        "                    continue\n",
        "                new_msg = f\"{old_msg}{model_think.split('</think>')[0]} {IGNORE_STR} \"\n",
        "                batch_messages[0][batch_idx][\"new_msg\"] = new_msg\n",
        "                print(f\"{batch_idx}: {new_msg}\")\n",
        "        num_ignores += 1\n",
        "\n",
        "    all_outputs.extend(batch_output_text)\n",
        "    print(f\"Processed batch {i//BSZ + 1}/{(len(messages) + BSZ - 1)//BSZ}\")\n",
        "\n",
        "for input_example, model_output in zip(data,all_outputs):\n",
        "    original_output = model_output\n",
        "    ground_truth = input_example['solution']\n",
        "    model_answer = extract_option_answer(original_output)\n",
        "\n",
        "    # Create a result dictionary for this example\n",
        "    result = {\n",
        "        'id': input_example['id'],\n",
        "        'question': input_example,\n",
        "        'ground_truth': ground_truth,\n",
        "        'model_output': original_output,\n",
        "        'extracted_answer': model_answer\n",
        "    }\n",
        "    final_output.append(result)\n",
        "\n",
        "    ground_truth_pattern = r'<answer>\\s*(\\w+)\\s*</answer>'\n",
        "    ground_truth_match = re.search(ground_truth_pattern, ground_truth)\n",
        "    ground_truth = ground_truth_match.group(1)\n",
        "\n",
        "    # Count correct answers\n",
        "    print(f\"model_answer: {model_answer}, ground_truth: {ground_truth}\")\n",
        "    if model_answer is not None and model_answer == ground_truth:\n",
        "        correct_number += 1\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = (correct_number / len(all_outputs)) * 100\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save results to a JSON file\n",
        "output_path = OUTPUT_PATH\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump({\n",
        "        'accuracy': accuracy,\n",
        "        'results': final_output\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 3. Get the filename\n",
        "filename = output_path\n",
        "\n",
        "# 4. Specify the destination path\n",
        "destination_path = os.path.join('/content/drive/My Drive/Colab_Output', filename)\n",
        "\n",
        "# 5. Move the uploaded file to the destination\n",
        "shutil.move(filename, destination_path)\n",
        "\n",
        "print(f'File \"{filename}\" uploaded to \"{destination_path}\" successfully.')\n"
      ],
      "metadata": {
        "id": "GRsBs7j62s9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}